{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79C8eWKB1P4M"
   },
   "source": [
    "# Topik Modelling Dengan Latent Semantic Indexing (LSI) atau Latent Semantic Analysis (LSA) menggunakan Scikit-Learn\n",
    "Dalam pembahasan kali ini, kita akan fokus pada Latent Semantic Indexing (LSI) atau Latent Semantic Analysis (LSA) dan melakukan topik modelling menggunakan Scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxJVH7Ha23vO"
   },
   "source": [
    "## **Topik Modelling**\n",
    "Topik Modelling ialah teknik tanpa pengawasan untuk menemukan tema dokumen yang diberikan. Ini mengekstrak kumpulan kata kunci yang terjadi bersama. Kata kunci yang muncul bersama ini mewakili sebuah topik. Misalnya, saham, pasar, ekuitas, reksa dana akan mewakili topik 'investasi saham'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysFKklwn3F_H"
   },
   "source": [
    "## **Latent Semantic Indexing (LSI) atau Latent Semantic Analysis (LSA)**\n",
    "Latent Semantic Indexing (LSI) atau Latent Semantic Analysis (LSA)  adalah teknik dalam natural language processing , khususnya  distributional semantics , yang menganalisis hubungan antara satu set dokumen dan istilah yang dikandungnya dengan menghasilkan satu set konsep yang terkait dengan dokumen dan istilah. LSA mengasumsikan bahwa kata-kata yang memiliki makna yang dekat akan muncul dalam potongan teks yang serupa (  distributional hypothesis ). Sebuah matriks yang berisi jumlah kata per dokumen (baris mewakili kata-kata unik dan kolom mewakili setiap dokumen) dibangun dari sepotong besar teks dan teknik matematika yang disebut Singular Value Decomposition (SVD) digunakan untuk mengurangi jumlah baris dengan tetap menjaga kesamaan struktur antar kolom. Dokumen kemudian dibandingkan dengan mengambil kosinus sudut antara dua vektor (atau produk titik antara normalisasi dua vektor) yang dibentuk oleh dua kolom. Nilai yang mendekati 1 menunjukkan dokumen yang sangat mirip sedangkan nilai yang mendekati 0 menunjukkan dokumen yang sangat berbeda. Untuk melakukan LSA dapat dilakukan dengan mengikuti tahapan tahapan berikut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXtSvKfs3oiP"
   },
   "source": [
    "## **Mengambil Dokumen**\n",
    "Langkah awal untuk melakukan Topik Modelling ialah dengan mengambil dokumen tersebut dengan mengcrawling data dokumen dengan menggunakan scrapy & crochet seperti berikut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kBDcPqq_nXi_",
    "outputId": "3640c6b3-7c95-4c7d-94c5-b0b475da7c8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (2.6.2)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (0.7.0)\n",
      "Requirement already satisfied: service-identity>=16.0.0 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (21.1.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (37.0.4)\n",
      "Requirement already satisfied: zope.interface>=4.1.3 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (5.4.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (1.0.6)\n",
      "Requirement already satisfied: protego>=0.1.15 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (0.2.1)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (22.0.0)\n",
      "Requirement already satisfied: tldextract in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (3.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (41.2.0)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (2.0.5)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (1.1.0)\n",
      "Requirement already satisfied: Twisted>=17.9.0 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (22.4.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (1.6.0)\n",
      "Requirement already satisfied: lxml>=3.5.0 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (4.9.1)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (2.0.1)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scrapy) (1.6.2)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cryptography>=2.0->scrapy) (1.14.5)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
      "Requirement already satisfied: six>=1.6.0 in c:\\users\\hamba notebook\\appdata\\roaming\\python\\python38\\site-packages (from parsel>=1.5.0->scrapy) (1.15.0)\n",
      "Requirement already satisfied: pyasn1 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from service-identity>=16.0.0->scrapy) (20.3.0)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
      "Requirement already satisfied: incremental>=21.3.0 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (21.3.0)\n",
      "Requirement already satisfied: constantly>=15.1 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (15.1.0)\n",
      "Requirement already satisfied: Automat>=0.8.0 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (20.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (4.3.0)\n",
      "Requirement already satisfied: twisted-iocpsupport<2,>=1.0.2 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (1.0.2)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tldextract->scrapy) (1.5.1)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tldextract->scrapy) (2.28.1)\n",
      "Requirement already satisfied: idna in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tldextract->scrapy) (3.3)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tldextract->scrapy) (3.8.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cffi>=1.12->cryptography>=2.0->scrapy) (2.20)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2022.6.15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: crochet in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: Twisted>=16.0 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from crochet) (22.4.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\hamba notebook\\appdata\\roaming\\python\\python38\\site-packages (from crochet) (1.12.1)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from Twisted>=16.0->crochet) (21.0.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from Twisted>=16.0->crochet) (20.3.0)\n",
      "Requirement already satisfied: Automat>=0.8.0 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from Twisted>=16.0->crochet) (20.2.0)\n",
      "Requirement already satisfied: constantly>=15.1 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from Twisted>=16.0->crochet) (15.1.0)\n",
      "Requirement already satisfied: twisted-iocpsupport<2,>=1.0.2 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from Twisted>=16.0->crochet) (1.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from Twisted>=16.0->crochet) (4.3.0)\n",
      "Requirement already satisfied: zope.interface>=4.4.2 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from Twisted>=16.0->crochet) (5.4.0)\n",
      "Requirement already satisfied: incremental>=21.3.0 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from Twisted>=16.0->crochet) (21.3.0)\n",
      "Requirement already satisfied: six in c:\\users\\hamba notebook\\appdata\\roaming\\python\\python38\\site-packages (from Automat>=0.8.0->Twisted>=16.0->crochet) (1.15.0)\n",
      "Requirement already satisfied: idna>=2.5 in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from hyperlink>=17.1.1->Twisted>=16.0->crochet) (3.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from zope.interface>=4.4.2->Twisted>=16.0->crochet) (41.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy\n",
    "!pip install crochet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MjU0dNBRndL-"
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "import re\n",
    "from crochet import setup, wait_for\n",
    "setup()\n",
    "\n",
    "class QuotesToCsv(scrapy.Spider):\n",
    "    name = \"MJKQuotesToCsv\"\n",
    "    start_urls = [\n",
    "        'https://nasional.tempo.co/read/1663729/uji-kelayakan-panglima-tni-gelak-tawa-pecah-saat-yudo-margono-tunjukkan-foto-istri'\n",
    "    ]\n",
    "    custom_settings = {\n",
    "        'ITEM_PIPELINES': {\n",
    "            '__main__.ExtractFirstLine': 1\n",
    "        },\n",
    "        'FEEDS': {\n",
    "            'news.csv': {\n",
    "                'format': 'csv',\n",
    "                'overwrite': True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        \"\"\"parse data from urls\"\"\"\n",
    "        for quote in response.css('#isi > p'):\n",
    "            yield {'news': quote.extract()}\n",
    "\n",
    "\n",
    "class ExtractFirstLine(object):\n",
    "    def process_item(self, item, spider):\n",
    "        \"\"\"text processing\"\"\"\n",
    "        lines = dict(item)[\"news\"].splitlines()\n",
    "        first_line = self.__remove_html_tags__(lines[0])\n",
    "\n",
    "        return {'news': first_line}\n",
    "\n",
    "    def __remove_html_tags__(self, text):\n",
    "        \"\"\"remove html tags from string\"\"\"\n",
    "        html_tags = re.compile('<.*?>')\n",
    "        return re.sub(html_tags, '', text)\n",
    "\n",
    "@wait_for(10)\n",
    "def run_spider():\n",
    "    \"\"\"run spider with MJKQuotesToCsv\"\"\"\n",
    "    crawler = CrawlerRunner()\n",
    "    d = crawler.crawl(QuotesToCsv)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hLvP-t2ISp3c",
    "outputId": "bb6df657-a94d-432a-9199-811eb775aaad"
   },
   "outputs": [],
   "source": [
    "run_spider()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jnEnUou4Nnc"
   },
   "source": [
    "## **Meload Dokumen**\n",
    "Setelah tahapan mengambil dokumen selesai, selanjutnya meload dokumen yang sudah didapatkan. Untuk dapat meload dokumen kita gunakan library os dan pandas seperti berikut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4McryBFvteQn"
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 1003: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-2d2063b45fe3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdocuments_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"news.csv\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mdocuments_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hamba notebook\\appdata\\local\\programs\\python\\python38\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 1003: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load Dataset\n",
    "documents_list = []\n",
    "with open( os.path.join(\"news.csv\") ,\"r\") as fin:\n",
    "    for line in fin.readlines():\n",
    "        text = line.strip()\n",
    "        documents_list.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epO6fJPE1B0s"
   },
   "source": [
    "## **Membuat Fitur TF-IDF**\n",
    "Setelah berhasil meload dokumen langkah selanjutnya ialah mengenerate fitur TF-IDF pada dokumen. Pada proses ini juga dilakukan operasi prepocessing, yaitu case folding, stopword, dan tokenizing. Untuk melakukan proses ini dengan menggunakan RegexpTokenizer dari library nltk seperti source code berikut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6hof42h0tz0S",
    "outputId": "b39b9fbb-c5ad-43f6-ba5f-6fab0e0acbf4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<12x236 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 412 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize regex tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Vectorize document using TF-IDF\n",
    "tfidf = TfidfVectorizer(lowercase=True,\n",
    "                        stop_words='english',\n",
    "                        ngram_range = (1,1),\n",
    "                        tokenizer = tokenizer.tokenize)\n",
    "\n",
    "# Fit and Transform the documents\n",
    "train_data = tfidf.fit_transform(documents_list)  \n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yw_YCNyu1O5_"
   },
   "source": [
    "## **Membuat Matrik SVD**\n",
    "Matrik SVD adalah teknik dekomposisi matriks yang memfaktorkan matriks dalam produk matriks. Untuk dapat membuat matrik tersebut kita dapat menggunakan TruncatedSVD dari library sklearn seperti berikut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QjAGqG9Ntnyv",
    "outputId": "5ff72ca1-6732-483a-810c-d45490ac8c1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.02366178e-02, -1.66382861e-02,  3.00062190e-02, ...,\n",
       "         1.35817657e-01, -1.98807570e-03,  7.74902033e-03],\n",
       "       [ 8.38985795e-02,  2.10631147e-01, -4.69070221e-02, ...,\n",
       "         1.82148827e-02, -2.80810446e-02, -1.39858160e-01],\n",
       "       [ 2.10849282e-02, -1.84815299e-02,  2.61033720e-03, ...,\n",
       "        -3.35817930e-02, -4.12961820e-04, -4.39328074e-03],\n",
       "       ...,\n",
       "       [ 2.52767786e-02, -2.00135859e-02, -5.03940302e-02, ...,\n",
       "        -3.07241369e-02,  1.88065252e-04, -2.37049787e-02],\n",
       "       [ 1.58110182e-02, -2.27024106e-02,  5.80867655e-02, ...,\n",
       "        -2.65079399e-02, -1.14091743e-02, -1.09058826e-02],\n",
       "       [ 2.27840084e-01, -1.53073584e-01, -1.05864584e-03, ...,\n",
       "        -4.25488190e-02, -4.07617741e-03,  7.17080829e-02]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "# Define the number of topics or components\n",
    "num_components=12\n",
    "\n",
    "# Create SVD object\n",
    "lsa = TruncatedSVD(n_components=num_components, n_iter=100, random_state=42)\n",
    "\n",
    "# Fit SVD model on data\n",
    "lsa.fit_transform(train_data)\n",
    "\n",
    "# Get Singular values and Components \n",
    "Sigma = lsa.singular_values_  \n",
    "V_transpose = lsa.components_.T\n",
    "V_transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PUxh1u21SxN"
   },
   "source": [
    "## **Ekstrak topik dan istilah**\n",
    "Setelah membuar matriks SVD, Selanjutnya kita perlu mengekstrak topik dari matriks komponen SVD dengan source code seperti berikut. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2IJSROA_uH7T",
    "outputId": "cdc418f0-a1e3-4b35-ffa1-eb1dd9fa30eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:twisted:/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: builtins.FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  ['negara', 'dan', 'yang', 'di', 'dalam']\n",
      "Topic 1:  ['elemen', 'ilmu', 'kunci', 'pengetahuan', 'adalah']\n",
      "Topic 2:  ['kemudian', 'juga', 'pendekatan', 'disebut', 'berbagai']\n",
      "Topic 3:  ['news', 'akan', 'sistem', 'pendekatan', 'kemudian']\n",
      "Topic 4:  ['bahan', 'korea', 'sangat', 'selatan', 'taiwan']\n",
      "Topic 5:  ['hal', 'baik', 'ini', 'cendekiawan', 'diskursus']\n",
      "Topic 6:  ['seringkali', 'banyak', 'sektor', 'cendekiawan', 'diskursus']\n",
      "Topic 7:  ['dan', 'juga', 'nies', 'antaranya', 'kelembagaan']\n",
      "Topic 8:  ['dokumen', 'bahan', 'korea', 'sangat', 'selatan']\n",
      "Topic 9:  ['ekonomi', 'bagian', 'begitu', 'berupaya', 'catch']\n",
      "Topic 10:  ['halnya', 'indonesia', 'langsung', 'menerapkan', 'mengabsorbsi']\n",
      "Topic 11:  ['amerika', 'austria', 'bagi', 'bahasa', 'belanda']\n"
     ]
    }
   ],
   "source": [
    "# Print the topics with their terms\n",
    "terms = tfidf.get_feature_names()\n",
    "\n",
    "for index, component in enumerate(lsa.components_):\n",
    "    zipped = zip(terms, component)\n",
    "    top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:5]\n",
    "    top_terms_list=list(dict(top_terms_key).keys())\n",
    "    print(\"Topic \"+str(index)+\": \",top_terms_list)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}